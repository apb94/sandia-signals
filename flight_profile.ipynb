{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports \n",
    "import os\n",
    "import h5py\n",
    "import time\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's look at effect of flight profile factors in isolation\n",
    "### Set-up - Define file location\n",
    "filename = 'N-CMAPSS_DS02-006.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Operation time (min):  0.03138361666666666\n",
      "\n",
      "W shape: (6517190, 4)\n",
      "X_s shape: (6517190, 14)\n",
      "X_v shape: (6517190, 14)\n",
      "T shape: (6517190, 10)\n",
      "A shape: (6517190, 4)\n"
     ]
    }
   ],
   "source": [
    "# Time tracking, Operation time (min):  0.003\n",
    "t = time.process_time()  \n",
    "\n",
    "# Load data\n",
    "with h5py.File(filename, 'r') as hdf:\n",
    "        # Development set\n",
    "        W_dev = np.array(hdf.get('W_dev'))             # W\n",
    "        X_s_dev = np.array(hdf.get('X_s_dev'))         # X_s\n",
    "        X_v_dev = np.array(hdf.get('X_v_dev'))         # X_v\n",
    "        T_dev = np.array(hdf.get('T_dev'))             # T\n",
    "        Y_dev = np.array(hdf.get('Y_dev'))             # RUL  \n",
    "        A_dev = np.array(hdf.get('A_dev'))             # Auxiliary\n",
    "\n",
    "        # Test set\n",
    "        W_test = np.array(hdf.get('W_test'))           # W\n",
    "        X_s_test = np.array(hdf.get('X_s_test'))       # X_s\n",
    "        X_v_test = np.array(hdf.get('X_v_test'))       # X_v\n",
    "        T_test = np.array(hdf.get('T_test'))           # T\n",
    "        Y_test = np.array(hdf.get('Y_test'))           # RUL  \n",
    "        A_test = np.array(hdf.get('A_test'))           # Auxiliary\n",
    "        \n",
    "        # Varnams\n",
    "        W_var = np.array(hdf.get('W_var'))\n",
    "        X_s_var = np.array(hdf.get('X_s_var'))  \n",
    "        X_v_var = np.array(hdf.get('X_v_var')) \n",
    "        T_var = np.array(hdf.get('T_var'))\n",
    "        A_var = np.array(hdf.get('A_var'))\n",
    "        \n",
    "        # from np.array to list dtype U4/U5\n",
    "        W_var = list(np.array(W_var, dtype='U20'))\n",
    "        X_s_var = list(np.array(X_s_var, dtype='U20'))  \n",
    "        X_v_var = list(np.array(X_v_var, dtype='U20')) \n",
    "        T_var = list(np.array(T_var, dtype='U20'))\n",
    "        A_var = list(np.array(A_var, dtype='U20'))\n",
    "                          \n",
    "W = np.concatenate((W_dev, W_test), axis=0)  \n",
    "X_s = np.concatenate((X_s_dev, X_s_test), axis=0)\n",
    "X_v = np.concatenate((X_v_dev, X_v_test), axis=0)\n",
    "T = np.concatenate((T_dev, T_test), axis=0)\n",
    "Y = np.concatenate((Y_dev, Y_test), axis=0) \n",
    "A = np.concatenate((A_dev, A_test), axis=0) \n",
    "    \n",
    "print('')\n",
    "print(\"Operation time (min): \" , (time.process_time()-t)/60)\n",
    "print('')\n",
    "print (\"W shape: \" + str(W.shape))\n",
    "print (\"X_s shape: \" + str(X_s.shape))\n",
    "print (\"X_v shape: \" + str(X_v.shape))\n",
    "print (\"T shape: \" + str(T.shape))\n",
    "print (\"A shape: \" + str(A.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t=np.concatenate((A[:,:2],T),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_w=np.concatenate((A[:,:2],W),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#approach 3: sliding window across all flights and units, #kernel samples from each flight\n",
    "def reshape(arr,kernel,window): #recall this was built with X_c in mind!!\n",
    "    #data points are sliding windows size w\n",
    "    #first grab number of flights\n",
    "    f=0\n",
    "    for i in np.unique(arr[:,0]):\n",
    "        dub=arr[arr[:,0]==i]\n",
    "        f+=np.max(dub[:,1])\n",
    "    trim=arr[:,2:] #get rid of indices\n",
    "    k=trim.shape[1] #only after you've deleted unnecessary features!!!;\n",
    "    X=np.zeros((int(kernel*f),window,k))\n",
    "    y=np.zeros((int(kernel*f),))\n",
    "    t_ticker=0\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    #indexer=[2,4,6,13,17,28]\n",
    "    for i,n in enumerate(np.unique(arr[:,0])):\n",
    "        dub=arr[arr[:,0]==n] #unit\n",
    "        for j in np.unique(dub[:,1]):\n",
    "            bub=dub[dub[:,1]==j] #flight\n",
    "            sub=MinMaxScaler().fit_transform(bub)\n",
    "            #sub=dub[dub[:,1]==j]\n",
    "            #t_tocker=0\n",
    "            t_tocker=int(len(sub)//2-kernel//2)\n",
    "            for k in range(kernel):\n",
    "                rub=sub[t_tocker:(t_tocker+window),:]\n",
    "                X[t_ticker,:,:]+=rub[:,2:] #should be dim (window,k)\n",
    "                y[t_ticker]+=len(sub)-(t_tocker+window)\n",
    "                t_ticker+=1\n",
    "                t_tocker+=1\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=reshape(X_w,kernel=100,window=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports for LSTM\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.metrics import MeanSquaredError,R2Score\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr=X\n",
    "y_tr=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.arange(len(X_tr))\n",
    "tr_ind=np.random.randint(0,len(X_tr),int(0.8*len(X_tr)))\n",
    "te_ind=np.delete(a,tr_ind)\n",
    "X_train=X_tr[tr_ind]\n",
    "y_train=y_tr[tr_ind]\n",
    "X_test=X_tr[te_ind]\n",
    "y_test=y_tr[te_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1134/1134 [==============================] - 39s 34ms/step - loss: 5556395.0000 - r2_score: -0.2632 - val_loss: 4377799.5000 - val_r2_score: -4.9770e-04\n",
      "Epoch 2/25\n",
      "1134/1134 [==============================] - 39s 34ms/step - loss: 4195027.0000 - r2_score: 0.0463 - val_loss: 2442536.2500 - val_r2_score: 0.4418\n",
      "Epoch 3/25\n",
      "1134/1134 [==============================] - 39s 34ms/step - loss: 1949032.2500 - r2_score: 0.5569 - val_loss: 1426321.0000 - val_r2_score: 0.6740\n",
      "Epoch 4/25\n",
      "1134/1134 [==============================] - 37s 32ms/step - loss: 1567150.3750 - r2_score: 0.6437 - val_loss: 1298904.0000 - val_r2_score: 0.7031\n",
      "Epoch 5/25\n",
      "1134/1134 [==============================] - 37s 33ms/step - loss: 1238511.1250 - r2_score: 0.7184 - val_loss: 1120481.7500 - val_r2_score: 0.7439\n",
      "Epoch 6/25\n",
      "1134/1134 [==============================] - 38s 34ms/step - loss: 1123384.5000 - r2_score: 0.7446 - val_loss: 978444.6875 - val_r2_score: 0.7764\n",
      "Epoch 7/25\n",
      "1134/1134 [==============================] - 38s 34ms/step - loss: 1049358.2500 - r2_score: 0.7614 - val_loss: 948887.1875 - val_r2_score: 0.7831\n",
      "Epoch 8/25\n",
      "1134/1134 [==============================] - 37s 33ms/step - loss: 969658.3750 - r2_score: 0.7796 - val_loss: 918772.4375 - val_r2_score: 0.7900\n",
      "Epoch 9/25\n",
      "1134/1134 [==============================] - 37s 33ms/step - loss: 887003.6875 - r2_score: 0.7983 - val_loss: 912498.6875 - val_r2_score: 0.7915\n",
      "Epoch 10/25\n",
      "1134/1134 [==============================] - 37s 33ms/step - loss: 855156.6250 - r2_score: 0.8056 - val_loss: 968344.1875 - val_r2_score: 0.7787\n",
      "Epoch 11/25\n",
      "1134/1134 [==============================] - 37s 33ms/step - loss: 798235.9375 - r2_score: 0.8185 - val_loss: 900914.1875 - val_r2_score: 0.7941\n",
      "Epoch 12/25\n",
      "1134/1134 [==============================] - 37s 33ms/step - loss: 1004464.0000 - r2_score: 0.7716 - val_loss: 993768.2500 - val_r2_score: 0.7729\n",
      "Epoch 13/25\n",
      "1134/1134 [==============================] - 37s 33ms/step - loss: 912694.0625 - r2_score: 0.7925 - val_loss: 988859.9375 - val_r2_score: 0.7740\n",
      "Epoch 14/25\n",
      "1134/1134 [==============================] - 37s 33ms/step - loss: 817389.3750 - r2_score: 0.8142 - val_loss: 665898.5625 - val_r2_score: 0.8478\n",
      "Epoch 15/25\n",
      "1134/1134 [==============================] - 37s 32ms/step - loss: 746773.2500 - r2_score: 0.8302 - val_loss: 651164.1875 - val_r2_score: 0.8512\n",
      "Epoch 16/25\n",
      "1134/1134 [==============================] - 36s 32ms/step - loss: 964807.0000 - r2_score: 0.7807 - val_loss: 1039715.6250 - val_r2_score: 0.7624\n",
      "Epoch 17/25\n",
      "1134/1134 [==============================] - 37s 32ms/step - loss: 900831.1250 - r2_score: 0.7952 - val_loss: 722432.8125 - val_r2_score: 0.8349\n",
      "Epoch 18/25\n",
      "1134/1134 [==============================] - 37s 32ms/step - loss: 736023.8750 - r2_score: 0.8327 - val_loss: 693198.5625 - val_r2_score: 0.8416\n",
      "Epoch 19/25\n",
      "1134/1134 [==============================] - 38s 34ms/step - loss: 717311.0000 - r2_score: 0.8369 - val_loss: 664810.0000 - val_r2_score: 0.8481\n",
      "Epoch 20/25\n",
      "1134/1134 [==============================] - 38s 33ms/step - loss: 656572.1250 - r2_score: 0.8507 - val_loss: 594547.6250 - val_r2_score: 0.8641\n",
      "Epoch 21/25\n",
      "1134/1134 [==============================] - 38s 34ms/step - loss: 603716.1250 - r2_score: 0.8628 - val_loss: 504352.7188 - val_r2_score: 0.8847\n",
      "Epoch 22/25\n",
      "1134/1134 [==============================] - 40s 35ms/step - loss: 605417.0000 - r2_score: 0.8624 - val_loss: 606668.1875 - val_r2_score: 0.8614\n",
      "Epoch 23/25\n",
      "1134/1134 [==============================] - 40s 35ms/step - loss: 572671.3750 - r2_score: 0.8698 - val_loss: 466777.9062 - val_r2_score: 0.8933\n",
      "Epoch 24/25\n",
      "1134/1134 [==============================] - 38s 34ms/step - loss: 586005.3750 - r2_score: 0.8668 - val_loss: 522446.7500 - val_r2_score: 0.8806\n",
      "Epoch 25/25\n",
      "1134/1134 [==============================] - 38s 34ms/step - loss: 543228.1250 - r2_score: 0.8765 - val_loss: 692082.4375 - val_r2_score: 0.8418\n"
     ]
    }
   ],
   "source": [
    "#construct RNN\n",
    "model=Sequential() #each xi is run of n timestamps and we are targeting RUL for each \n",
    "model.add(LSTM(100,return_sequences=True,input_shape=(X_train.shape[1],X_train.shape[2]))) #input layer\n",
    "model.add(LSTM(100,return_sequences=False)) #hidden layer\n",
    "model.add(Dense(100,'relu')) #activation function\n",
    "model.add(Dense(1,'linear')) #output\n",
    "cp=ModelCheckpoint('modelnew.h5',save_best_only=True) #to save the best model\n",
    "model.compile(loss='MeanSquaredError',optimizer=Adam(learning_rate=0.01),metrics=['R2Score']) #set the loss function optimizer and metric\n",
    "model.fit(X_train,y_train,validation_split=0.3,epochs=25,callbacks=[cp]) #fit the model\n",
    "model=load_model('modelnew.h5')\n",
    "\n",
    "#X and y appear to be what I want them to be, scaled, matching.\n",
    "#something about the setup of this model?\n",
    "#worth a deep dive into the architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "908/908 [==============================] - 9s 10ms/step\n"
     ]
    }
   ],
   "source": [
    "preds=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's simulate two different routes KSEA-KJFK\n",
    "#route 1 direct 36k\n",
    "#route 2 around storm 38k, colder inlet temps, higher power settings, higher airspeed\n",
    "#we can use our autoencoder to build these routes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#autoencoder for early anomaly detection\n",
    "#let's focus on \"longer\" flights >5000 timestamps\n",
    "#add index columns to help in restructuring\n",
    "X_c=np.concatenate((A[:,:2],X_s,X_v),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we'll try univariate first\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "l=[]\n",
    "window=1000\n",
    "for i in np.unique(X_c[:,0]):\n",
    "    dub=X_c[X_c[:,0]==i]\n",
    "    for j in np.unique(dub[:,1]):\n",
    "        sub=MinMaxScaler().fit_transform(dub[dub[:,1]==j])\n",
    "        if len(sub)>3000 and len(sub)<5000:\n",
    "            #ind=np.linspace(0,len(sub),5001).astype(int)[:-1]\n",
    "            for k in range(2000):\n",
    "                l.append(sub[k:k+window,5])\n",
    "X=np.stack(l)\n",
    "X=X.reshape(X.shape[0],X.shape[1],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Input(shape=(X.shape[1], X.shape[2])),\n",
    "        layers.Conv1D(\n",
    "            filters=20,\n",
    "            kernel_size=8,\n",
    "            padding=\"same\",\n",
    "            strides=2,\n",
    "            activation=\"relu\",\n",
    "        ),\n",
    "        layers.Dropout(rate=0.2),\n",
    "        layers.Conv1D(\n",
    "            filters=20,\n",
    "            kernel_size=8,\n",
    "            padding=\"same\",\n",
    "            strides=2,\n",
    "            activation=\"relu\",\n",
    "        ),\n",
    "        layers.Conv1DTranspose(\n",
    "            filters=20,\n",
    "            kernel_size=8,\n",
    "            padding=\"same\",\n",
    "            strides=2,\n",
    "            activation=\"relu\",\n",
    "        ),\n",
    "        layers.Dropout(rate=0.2),\n",
    "        layers.Conv1DTranspose(\n",
    "            filters=20,\n",
    "            kernel_size=8,\n",
    "            padding=\"same\",\n",
    "            strides=2,\n",
    "            activation=\"relu\",\n",
    "        ),\n",
    "        layers.Conv1DTranspose(filters=1, kernel_size=10, padding=\"same\"),\n",
    "    ]\n",
    ")\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss=\"mae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "64/64 [==============================] - 4s 62ms/step - loss: 0.2122 - val_loss: 0.1138\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 4s 62ms/step - loss: 0.0860 - val_loss: 0.0258\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 4s 60ms/step - loss: 0.0308 - val_loss: 0.0335\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 4s 60ms/step - loss: 0.0256 - val_loss: 0.0351\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 4s 60ms/step - loss: 0.0234 - val_loss: 0.0282\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 4s 60ms/step - loss: 0.0222 - val_loss: 0.0324\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 4s 62ms/step - loss: 0.0209 - val_loss: 0.0300\n",
      "Epoch 8/10\n",
      "64/64 [==============================] - 4s 67ms/step - loss: 0.0201 - val_loss: 0.0291\n",
      "Epoch 9/10\n",
      "64/64 [==============================] - 4s 63ms/step - loss: 0.0236 - val_loss: 0.0263\n",
      "Epoch 10/10\n",
      "64/64 [==============================] - 4s 62ms/step - loss: 0.0178 - val_loss: 0.0393\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X,\n",
    "    X,\n",
    "    epochs=10,\n",
    "    batch_size=300,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, mode=\"min\")\n",
    "    ],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
